{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyNem3N/DYJlkaqCuj2VGWtO"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"4o5WX8giVL5J"},"outputs":[],"source":["!pip install torch transformers datasets\n"]},{"cell_type":"code","source":["import torch, torch.nn as nn, torch.nn.functional as F\n","from torch.utils.data import DataLoader\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset"],"metadata":{"id":"T_KfQckPVw9Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["teacher_name = \"gpt2\"\n","teacher = AutoModelForCausalLM.from_pretrained(teacher_name)\n","tokenizer = AutoTokenizer.from_pretrained(teacher_name)\n","if tokenizer.pad_token is None:\n","    tokenizer.pad_token = tokenizer.eos_token\n","teacher.eval()\n","for p in teacher.parameters():\n","    p.requires_grad = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RPF1n-WuVy_c","executionInfo":{"status":"ok","timestamp":1759853597501,"user_tz":-120,"elapsed":31301,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"a6affaf9-5589-452a-bb2d-5281caf045cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["class TinyTransformerLM(nn.Module):\n","    def __init__(self, vocab_size, d_model=256, nhead=4, num_layers=2):\n","        super().__init__()\n","        self.embed = nn.Embedding(vocab_size, d_model)\n","        self.pos_embed = nn.Embedding(512, d_model)\n","        encoder_layer = nn.TransformerEncoderLayer(d_model, nhead, dim_feedforward=512)\n","        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers)\n","        self.lm_head = nn.Linear(d_model, vocab_size)\n","\n","    def forward(self, input_ids):\n","        B, L = input_ids.shape\n","        pos = torch.arange(0, L, device=input_ids.device).unsqueeze(0).expand(B, L)\n","        x = self.embed(input_ids) + self.pos_embed(pos)\n","        x = self.transformer(x)               # [B, L, d_model]\n","        logits = self.lm_head(x)\n","        return logits"],"metadata":{"id":"MvArISA-V7Sw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["vocab_size = tokenizer.vocab_size\n","student = TinyTransformerLM(vocab_size)\n","vanilla_student = TinyTransformerLM(vocab_size)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vkm5R8S8V--u","executionInfo":{"status":"ok","timestamp":1759853680516,"user_tz":-120,"elapsed":1019,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"02c82825-1b24-4e9b-8e41-ba7e0f514b84"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/torch/nn/modules/transformer.py:392: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n","  warnings.warn(\n"]}]},{"cell_type":"code","source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)\n","teacher.to(device)\n","student.to(device)\n","vanilla_student.to(device)\n","print(\"---\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a3AwZOwZWY_0","executionInfo":{"status":"ok","timestamp":1759853700928,"user_tz":-120,"elapsed":255,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"db16c3ee-465a-4088-9b1f-f9ab2cfb1c22"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n","---\n"]}]},{"cell_type":"code","source":["ds = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"train[:1%]\")  # small slice\n","def tok_fn(ex):\n","    return tokenizer(ex[\"text\"], truncation=True, padding=\"max_length\", max_length=64)\n","ds = ds.map(tok_fn, batched=True)\n","ds.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\"])\n","dl = DataLoader(ds, batch_size=8, shuffle=True)\n"],"metadata":{"id":"GkW14FrdV_U2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def count_parameters(model):\n","    total = sum(p.numel() for p in model.parameters())\n","    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n","    print(f\"Total parameters: {total:,}\")\n","    print(f\"Trainable parameters: {trainable:,}\")\n","    return total, trainable\n","count_parameters(teacher), count_parameters(student)\n","print(\"--\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fk_MBTeSWnPO","executionInfo":{"status":"ok","timestamp":1759853705925,"user_tz":-120,"elapsed":24,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"4087b316-7d62-481e-d4a1-294f9048df2d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Total parameters: 124,439,808\n","Trainable parameters: 0\n","Total parameters: 26,967,121\n","Trainable parameters: 26,967,121\n","--\n"]}]},{"cell_type":"code","source":["optimizer = torch.optim.Adam(student.parameters(), lr=1e-4)\n","T = 2.0\n","alpha = 0.5"],"metadata":{"id":"0brCxWQLaL5w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Given the total loss function:\n","\n","$$\n","L_{\\text{total}} = \\alpha L_{\\text{CE}} + (1 - \\alpha) L_{\\text{KL}}\n","$$\n","\n","The gradient with respect to the model parameters $\\theta$ is:\n","\n","$$\n","\\nabla_\\theta L_{\\text{total}}\n","= \\alpha \\nabla_\\theta L_{\\text{CE}}\n","+ (1 - \\alpha) \\nabla_\\theta L_{\\text{KL}}\n","$$\n","\n","$\\Rightarrow$ The update step depends on both components.Since the derivative of a sum is the sum of the derivatives, each term contributes proportionally to its weight ($\\alpha$ and $1 - \\alpha$).\n"],"metadata":{"id":"zKFcPtRfcH7s"}},{"cell_type":"code","source":["# the model is updating its weights with a double objective --> decrease the difference between its own (student) probability distribution overt the vocabulary and\n","# the distribution of the teacher (we can say the student is punished when its own probability distribution is too different from the one of the teacher) and\n","# at the same time it has some freedom to update the weights wrt its own loss --> the one that comes from its own prediction.\n","\n","# given Ltotal​=αLCE​+(1−α)LKL\n","# ∇θ​Ltotal​=α∇θ​LCE​+(1−α)∇θ​LKL --> update depends on both --> and we know that derivative of sum is sum of derivative\n","#\n","# One that says: “adjust yourself to reduce prediction errors vs the ground truth.”\n","# ∇CE points toward the minimum where you get correct labels.\n","#\n","# Another that says: “also align your output distribution with what the teacher believes.”\n","# ∇KL points toward the minimum where you look like the teacher.\n","\n","\n","for epoch in range(10):\n","    for batch in dl:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        labels = input_ids[:, 1:].contiguous()\n","        inputs = input_ids[:, :-1].contiguous()\n","\n","\n","\n","        with torch.no_grad():\n","            t_logits = teacher(inputs).logits  # [B, L, V]\n","        s_logits = student(inputs)\n","\n","        # Distillation loss\n","        t_probs = F.softmax(t_logits / T, dim=-1)\n","        s_log_probs = F.log_softmax(s_logits / T, dim=-1)\n","        kl = F.kl_div(s_log_probs, t_probs, reduction=\"batchmean\") * (T * T)\n","\n","        # Hard CE loss\n","        ce = F.cross_entropy(\n","            s_logits.view(-1, s_logits.size(-1)),\n","            labels.view(-1),\n","            ignore_index=tokenizer.pad_token_id,\n","        )\n","\n","        loss = alpha * ce + (1 - alpha) * kl\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f\"Epoch {epoch}: loss={loss.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"y6MVJzb6WGPI","executionInfo":{"status":"ok","timestamp":1759854116060,"user_tz":-120,"elapsed":35534,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"6de083ef-2c69-4a7b-d236-2c11339faff7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: loss=54.5110\n","Epoch 1: loss=43.7234\n","Epoch 2: loss=55.5211\n","Epoch 3: loss=18.7598\n","Epoch 4: loss=57.9055\n","Epoch 5: loss=40.7901\n","Epoch 6: loss=63.5930\n","Epoch 7: loss=27.3593\n","Epoch 8: loss=57.7921\n","Epoch 9: loss=27.0922\n"]}]},{"cell_type":"code","source":["optimizer = torch.optim.AdamW(student.parameters(), lr=1e-4)\n","alpha = 1"],"metadata":{"id":"zj8knjj9cgDD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for epoch in range(10):\n","    for batch in dl:\n","        input_ids = batch[\"input_ids\"].to(device)\n","        labels = input_ids[:, 1:].contiguous()\n","        inputs = input_ids[:, :-1].contiguous()\n","\n","\n","\n","        with torch.no_grad():\n","            t_logits = teacher(inputs).logits  # [B, L, V]\n","        s_logits = vanilla_student(inputs)\n","\n","        # Distillation loss\n","        t_probs = F.softmax(t_logits / T, dim=-1)\n","        s_log_probs = F.log_softmax(s_logits / T, dim=-1)\n","        kl = F.kl_div(s_log_probs, t_probs, reduction=\"batchmean\") * (T * T)\n","\n","        # Hard CE loss\n","        ce = F.cross_entropy(\n","            s_logits.view(-1, s_logits.size(-1)),\n","            labels.view(-1),\n","            ignore_index=tokenizer.pad_token_id,\n","        )\n","\n","        loss = alpha * ce + (1 - alpha) * kl\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    print(f\"Epoch {epoch}: loss={loss.item():.4f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aQwOA3wvcdc-","executionInfo":{"status":"ok","timestamp":1759854184079,"user_tz":-120,"elapsed":32047,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"a571bdf2-a192-428a-c3f4-1aa493066066"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 0: loss=10.9947\n","Epoch 1: loss=10.9750\n","Epoch 2: loss=10.9391\n","Epoch 3: loss=10.9614\n","Epoch 4: loss=11.0002\n","Epoch 5: loss=10.9829\n","Epoch 6: loss=11.0245\n","Epoch 7: loss=11.0257\n","Epoch 8: loss=10.9148\n","Epoch 9: loss=10.9978\n"]}]},{"cell_type":"code","source":["max_token = 10\n","prompt = \"Once upon a time\"\n","\n","for t in range(max_token):\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n","    out = teacher(input_ids)\n","\n","    next_token_logits = out.logits[:, -1, :] # batch, seq_len, vocab_size\n","\n","    next_token_id = next_token_logits.argmax(dim=-1) # take the greatest value of the logits in along the vocab_size\n","\n","    next_token = tokenizer.decode(next_token_id) # decode and add\n","    prompt += next_token\n","\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cUHN8dCrXkE8","executionInfo":{"status":"ok","timestamp":1759854210617,"user_tz":-120,"elapsed":114,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"b6bfd2b5-b83b-4a05-b6e3-ece78619880f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Once upon a time, the world was a place of great beauty and\n"]}]},{"cell_type":"code","source":["max_token = 10\n","prompt = \"Once upon a time\"\n","\n","for t in range(max_token):\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n","    out = vanilla_student(input_ids)\n","\n","    next_token_logits = out[:, -1, :] # batch, seq_len, vocab_size\n","    next_token_id = next_token_logits.argmax(dim=-1) # take the greatest value of the logits in along the vocab_size\n","\n","    next_token = tokenizer.decode(next_token_id) # decode and add\n","    prompt += next_token\n","\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AVafjbkrdkc_","executionInfo":{"status":"ok","timestamp":1759854212826,"user_tz":-120,"elapsed":34,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"45c325d8-dc5a-42a9-be29-ae59334eb182"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Once upon a timews repeatedlyAutabyte statue circumcision Colourhesis regarded Expansion\n"]}]},{"cell_type":"code","source":["max_token = 10\n","prompt = \"Once upon a time\"\n","\n","for t in range(max_token):\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n","    out = student(input_ids)\n","\n","    next_token_logits = out[:, -1, :] # batch, seq_len, vocab_size\n","    next_token_id = next_token_logits.argmax(dim=-1) # take the greatest value of the logits in along the vocab_size\n","\n","    next_token = tokenizer.decode(next_token_id) # decode and add\n","    prompt += next_token\n","\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y7DH90xRWHf8","executionInfo":{"status":"ok","timestamp":1759854223063,"user_tz":-120,"elapsed":31,"user":{"displayName":"Piergiorgio Di Pasquale","userId":"07379998051964901818"}},"outputId":"24507a85-876b-43a0-a86c-cc7390c52776"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Once upon a time and the the the the the the the the the\n"]}]}]}